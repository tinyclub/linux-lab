---
title: 泰晓资讯·4月 / 第四期 / 2020
author: 'Wang Chen'
group: news
draft: false
top: false
album: 泰晓资讯
layout: weekly
license: "cc-by-nc-nd-4.0"
permalink: /tinylab-weekly-04-4th-2020/
tags:
  - Linux
  - split-lock
  - task-isolation
  - QEMU
  - VirtIO-FS
categories:
  - 泰晓资讯
  - 技术动态
  - 行业动向
---

**“泰晓资讯”，广泛报道 “Linux/开源” 业界资讯。欢迎广大读者投递相关资讯来源和素材，本站将进一步收集整理后发布给大家。**

- **5.7 合并窗口修改盘点**

    5.7 版本的合并窗口已经结束，截止目前有 11,998 个 non-merge 修改合入到了本开发周期的主线中。比 5.6 合并窗口期间多了 1218 个；看来此刻的全球性疫情并没有拖累内核社区的开发速度，至少现在还没有。这些补丁带来的修复、改进和新功能不少。我在这里摘取了一些比较大的改动，更多细节请参阅 [“5.7 Merge window part 1”](https://lwn.net/Articles/816313/) 和 [“5.7 Merge window part 2”](https://lwn.net/Articles/816934/)。

    - 体系架构相关

        - x86: 备受争议的 “split-lock detection” 补丁终于被合并了。目前发现，该补丁的引入会影响一些基于 VMware 的虚拟机的正常运行，本期资讯有文章介绍，详见下文 **“split-lock 检测” 补丁对 VMX 的影响**。
        - ARM 架构支持内存热移除（hot-removal）。
        - ARM 上的 “pointer authentication” 功能现在支持内核空间（用户空间的支持已经合入一段时间了）。
        - RISC-V 支持 CPU hotplug
        - 其他

    - 内核核心子系统

        - io_uring 子系统现支持了 `splice()` 和 “ automatic buffer selection”（具体参考 [“泰晓资讯·4月 / 第二期 / 2020”](https://tinylab.org/tinylab-weekly-04-2nd-2020/) 对 “io_uring 的发展新动向” 的介绍）
        - “thermal pressure” 补丁。调度器在调度任务时会考虑到处理器的温度影响。
        - CPU 调度器的负载跟踪（load tracking）在 x86 架构上终于实现了 “频率无关（frequency invariance）”。这意味着无论 CPU 当前的工作频率是什么，它都能获得正确的 utilization 值。
        - 在经历了相当多的讨论和修改之后，BPF 和 “实时抢占（realtime preemption）” 功能现在可以很好地共存了。
        - 新增的 `BPF_MODIFY_RETURN BPF` 程序类型可以附加到内核中的一个函数上并修改其返回值。
        - cgroup memory controller 实现了 "recursive memory.low protection"。
        - 支持使用 `clone3()` 在创建进程时直接配置好它属于哪个 cgroup。
        - 其他

    - 文件系统和块设备处理

        - Btrfs 文件系统提供了一个新的 `ioctl()`命令 (`BTRFS_IOC_SNAP_DESTROY_V2`)，可以通过 subvolume 的 ID 来删除 subvolume。
        - 移除旧的 exFAT 文件系统，替换为一个新的实现，该实现由三星提供。
        - F2FS 文件系统现在支持 zstd 压缩。
        - 其他

    - 网络

        - 网络层现在可以利用硬件卸载 802.11 封装任务的优势。
        - 新的 "Bareudp" 模块提供了通用的 level-3 UDP 封装。
        - 将设备从一个 network namespace 移动到另一个 network namespace 时现在会相应地调整相关的 sysfs 文件的属主和权限。
        - 继续合并 “multipath TCP” 补丁，但在主线中实现完整的 MPTCP 可能还需要一段时间。

    其他安全相关以及对新硬件的支持。

    后继的工作将集中于修复新代码中的 bug，如果不出意外，5.7 版本将在 5 月 31 日 或 6 月 7 日左右发布。

    **关键词**: Linux，5.7

- [**“split-lock 检测” 补丁对 VMX 的影响**](https://lwn.net/Articles/816918/)

    > One of the many features merged for the 5.7 kernel is split-lock detection for the x86 architecture. This feature has encountered a fair amount of controversy over the course of its development, with the result that the time between its initial posting and appearance in a released kernel will end up being over two years. As it happens, there is another hurdle for split-lock detection even after its merging into the mainline; this feature threatens to create problems for a number of virtualization solutions, and it's not clear what the solution would be.

    5.7 内核版本中会合入一个对 x86 影响比较大的功能 - “split-lock detection”。有必要简单了解一下，特别是对于那些从事云端服务和虚拟化的朋友。

    我们知道大多数处理器架构都不支持跨 cache-line 访问，但是 Intel 系却支持，看上去这是一项对应用开发人员友好的特性，但是却可能引来大-麻烦。特别是当应用要求以原子方式访问跨 cache-line 的内存数据过程中，处理器会短暂地锁住内存总线，这就是所谓的 split-lock。而这会带来系统整体上的性能损失，甚至在一些云服务器上会招致一些恶意应用的攻击。

    Intel 处理器的这个行为以前是不暴露给上层的，但最近 Intel 的新处理器系列 Tremont 提供了一项新功能，就是一旦检测到有跨 cache-line 的访问发生就会抛出异常。为此内核要做的就是配合这个硬件改进对异常执行相应的操作，这个就是所谓的 “split-lock detection” 补丁的来源。

    具体补丁要决定的无非就是针对该异常的处理策略问题，但正是这个如何做的问题，引起了社区的热烈讨论和补丁的反复修改，从发起修改到这次计划合入 5.7，时间跨度已有超过两年！但就在合入的这当口，一些来自虚拟化用户的问题又把这个补丁推到了风口浪尖上。

    看上去这个问题和 Intel 的 "virtual machine extensions" (简称 VMX, 我们也常称之为 "VT-x") 模式有关。当虚拟机利用 VMLAUNCH 指令启用硬件的这个模式，同时内核又启动了 “split-lock detection” 后，一旦虚拟机中的应用触发了 split-lock，异常处理会导致虚拟机被杀死。这引起了虚拟机用户的混乱。为此社区提出了一系列的补救措施，但看上去还有一些问题，具体的讨论请参阅原文 [“VMX virtualization runs afoul of split-lock detection”](https://lwn.net/Articles/816918/)。但无论如何 “split-lock detection” 这个补丁会在 5.7 中合入，也算是为这个有着两年悠久历史的补丁做个了结。

    **关键词**: Linux，split-lock detection

- [**“支持真正意义上的任务隔离（task-isolation）”**](https://lwn.net/Articles/816298/)

    > Some applications require guaranteed access to the CPU without even brief interruptions; realtime systems and high-bandwidth networking applications with user-space drivers can fall into the category. While Linux provides some support for CPU isolation (moving everything but the critical task off of one or more CPUs) now, it is an imperfect solution that is still subject to some interruptions. Work has been continuing in the community to improve the kernel's CPU-isolation capabilities, notably with improvements in the nohz (tickless) mode, but it is not finished yet. Recently, Alex Belits submitted a patch set (based on work by Chris Metcalf in 2015) that introduces a completely predictable environment for Linux applications — as long as they do not need any kernel services.

    有些应用希望一直拥有 CPU，甚至不允许被中断打断。譬如实时系统，还有那些以用户态驱动（user-space driver）方式工作的极度消耗带宽的网络应用程序都属于这种情况。

    我们目前可以利用内核中的 “CPU isolation（或称 "isolcpus"）” 以及 Nohz 功能在一定程度上支持以上需求，但都不是很完善。

    “isolcpus” 功能存在已久，上溯到 v2.6.11（2005年），那时内核就已经支持该功能。“isolcpus” 的作用是把一个或多个 CPU 隔离出来（所谓隔离是指调度器将不再管理这些 CPU），然后我们可以通过明确设置 affinity 来指定任务使用这些被隔离的 CPU。这么做的好处是在这些隔离的 CPU 上运行的进程就不需要与其他任务争夺 CPU 时间， 有效地提高了这些任务运行的实时性。

    将内核配置为 nohz 模式（3.10 合入）可以减少 CPU 接收到的中断数量（特别是时钟中断），然而，nohz 并不能保证完全没有中断；运行中的任务仍然可能会被 page fault 或 delayed workqueue 所打断。当然精心设计的应用程序其实可以避免这些情况，而且在 nohz 模式下的好处在于，任务仍可以执行常规代码，包括系统调用，而额外的时间开销基本都是在进入或者退出系统调用的路径上。

    总而言之，内核的以上两种特性并不足以满足 **完全没有中断** 下的用户态任务运行。为此最近 Alex Belits 提交了一个补丁（基于 Chris Metcalf 2015 年的一些工作），允许在代码不发起系统调用的前提下，一个任务可以在用户态充分地使用处理器，而不用考虑被中断打断，我们称这个功能叫 “任务隔离（task isolation）”。

    对大多数读者来说，了解以上背景应该就足够了，如果您希望更深入地学习如何使用该补丁以及该补丁的具体实现，可以参阅原文介绍 [“A full task-isolation mode for the kernel”](https://lwn.net/Articles/816298/)。这组补丁提交到社区后已经得到了不少正面评价，看来大家对这个功能还是蛮感兴趣的。这个补丁有很大概率会被合入后续的内核版本中。

    **关键词**: Linux，task isolation

- [**QEMU 5.0 将支持 VirtIO-FS，方便主机和虚拟机更好地共享文件**](https://www.phoronix.com/scan.php?page=news_item&px=VirtIO-FS-QEMU-5.0-Merged)

    > Added back in Linux 5.4 was the VirtIO-FS file-system driver as a a FUSE-framework-based file-system implementation designed for guest to/from host file-system sharing for VirtIO para-virtualized devices. Now with QEMU 5.0 VirtIO-FS is supported on its side.

    Linux 5.4 中添加了 VirtIO-FS 文件系统驱动程序，该驱动基于 FUSE 框架的文件系统实现，可用于 guest 和 host 之间基于 VirtIO 半虚拟化设备实现文件共享。现在，QEMU 5.0 也开始支持 VirtIO-FS。在 host 和 guest 之间共享文件/文件夹时，VirtIO-FS 提供比 VirtIO-9P 更好的性能。开发人员 Stefan Hajnoczi 在其本周发表的 [博客文章](http://blog.vmsplice.net/2020/04/virtio-fs-has-landed-in-qemu-50.html) 中概述了在内核配置中如何结合使用 VirtIO-FS 和 QEMU 5.0。

    **关键词**: QEMU，VirtIO-FS

## 联系我们

资讯内容部分来自 [“LWN.net“](https://lwn.net/)。如果您对某些 LWN 文章感兴趣（譬如希望获得全文翻译的）请扫描二维码加微信联系我们：

![tinylab wechat](/images/wechat/tinylab.jpg)
